import os
import streamlit as st
import chardet
from text_splitter import clean_text_and_split  # text_splitter.pyì—ì„œ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from prompt_config import SYSTEM_PROMPT
from apikey import API_KEY

os.environ["OPENAI_API_KEY"] = API_KEY

@st.cache_resource
def load_txt(raw_data):
    encoding_result = chardet.detect(raw_data)
    detected_encoding = encoding_result["encoding"]
    
    try:
        raw_text = raw_data.decode(detected_encoding)
    except UnicodeDecodeError:
        st.error(f"íŒŒì¼ì„ {detected_encoding} ì¸ì½”ë”©ìœ¼ë¡œ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        st.stop()
    
    st.success(f"âœ… {uploaded_file.name} íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤!")
    return raw_text
    
# í…ìŠ¤íŠ¸ ì •ì œ ë° ë¶„í• 
# TextSplitter ì—­í• 
def text_splitter(_raw_text):
    return clean_text_and_split(_raw_text) # Document ê°ì²´ return

# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ê°€ì ¸ì˜¤ê¸°
def get_system_prompt():
    return SYSTEM_PROMPT

# ë²¡í„° ì €ì¥ì†Œ ìƒì„± ë° ë¡œë“œ
@st.cache_resource
def create_vector_store(_raw_text):
    split_docs = text_splitter(_raw_text)
    persist_directory = "./chroma_db"
    vectorstore = Chroma.from_documents(
        split_docs, 
        OpenAIEmbeddings(model='text-embedding-3-small'),
        persist_directory=persist_directory
    )
    return vectorstore

@st.cache_resource
def get_vectorstore(_raw_text):
    persist_directory = "./chroma_db"
    if os.path.exists(persist_directory):
        return Chroma(
            persist_directory=persist_directory,
            embedding_function=OpenAIEmbeddings(model='text-embedding-3-small')
        )
    return create_vector_store(_raw_text)

# Document ê°ì²´ì˜ page_contentë¥¼ Join
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# RAG ì‹œìŠ¤í…œ ì„¤ì •
@st.cache_resource
def chaining(_raw_text):
    # ì—…ë¡œë“œí•œ ë°ì´í„°ê°€ ë‹´ê¸´ ë¦¬ìŠ¤íŠ¸ : raw_text
    vectorstore = get_vectorstore(_raw_text)
    
    # vectorstoreì—ì„œ ë¬¸ì„œë¥¼ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” retriever ê°ì²´ ìƒì„±
    # ê²€ìƒ‰ ìš”ì²­ì´ ë“¤ì–´ì˜¤ë©´ ë²¡í„° ìœ ì‚¬ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ë°˜í™˜
    retriever = vectorstore.as_retriever()

    # prompt_config.pyì—ì„œ SYSTEM_PROMPT ê°€ì ¸ì˜¤ê¸°
    qa_system_prompt = get_system_prompt()

    # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ì„ ê²°í•©
    qa_prompt = ChatPromptTemplate.from_messages(
        [
            ("system", qa_system_prompt),
            ("human", "{input}"),
        ]
    )

    # LLM ëª¨ë¸ ì´ˆê¸°í™”
    llm = ChatOpenAI(model="gpt-4o-mini")

    # RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
    rag_chain = (
        {"context": retriever | format_docs, "input": RunnablePassthrough()}
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    """
    retrieverë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
    format_docsë¡œ ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ë³€í™˜
    
    context: ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ LLMì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì „ë‹¬
    input: ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ê·¸ëŒ€ë¡œ ì „ë‹¬
    
    | qa_prompt: context(ê²€ìƒ‰ëœ ë¬¸ì„œ)ì™€ input(ì‚¬ìš©ì ì§ˆë¬¸)ì„ í”„ë¡¬í”„íŠ¸ì— ì‚½ì…
    | llm: GPT-4o-mini ëª¨ë¸ì´ ì‘ë‹µì„ ìƒì„±
    | StrOutputParser(): ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
    """

    return rag_chain # RAG ê²€ìƒ‰ + LLM ì‘ë‹µ ìƒì„± íŒŒì´í”„ë¼ì¸(rag_chain) ë°˜í™˜

# Streamlit UI
st.title("ğŸ’¬ Chatbot with File Upload")

# íŒŒì¼ ì—…ë¡œë“œ ê¸°ëŠ¥ ì¶”ê°€
uploaded_file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ (txt)", type=["txt"])

if uploaded_file is not None:
    file_extension = uploaded_file.name.split(".")[-1].lower()

    # íŒŒì¼ ì¸ì½”ë”© ê°ì§€ ë° ì½ê¸°
    raw_data = uploaded_file.read()
    raw_text = load_txt(raw_data)
    
    rag_chain = chaining(raw_text)
    vectorstore = get_vectorstore(raw_text) # ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ
    retriever = vectorstore.as_retriever() # RAGìš© ê²€ìƒ‰ ì—”ì§„

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    if prompt_message := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•´ì£¼ì„¸ìš” :)"):
        st.chat_message("human").write(prompt_message)
        st.session_state.messages.append({"role": "user", "content": prompt_message})
        
        # vectorStoreë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ ê²°ê³¼
        with st.expander("ğŸ” VectorStore ê²€ìƒ‰ ê²°ê³¼ ë³´ê¸°"):
            with st.spinner("Retrieving relevant documents..."):
                docs = vectorstore.similarity_search(prompt_message)
                st.write("ğŸ”¹ **ê²€ìƒ‰ëœ ë¬¸ì„œ:**")
                st.write(format_docs(docs))
        
        # RAGë§Œ ì‚¬ìš©í•œ ê²€ìƒ‰ ê²°ê³¼
        with st.expander("ğŸ” RAG ê²€ìƒ‰ ê²°ê³¼ ë³´ê¸°"):
            with st.spinner("Retrieving relevant documents..."):
                docs = retriever.get_relevant_documents(prompt_message)
                st.write("ğŸ”¹ **ê²€ìƒ‰ëœ ë¬¸ì„œ:**")
                st.write(format_docs(docs))
        
        # GPT í¬í•¨ RAG ê²°ê³¼ (ê¸°ì¡´ ë°©ì‹)
        with st.chat_message("ai"):
            with st.spinner("Thinking..."):
                response = rag_chain.invoke(prompt_message)
                st.session_state.messages.append({"role": "assistant", "content": response})
                st.write(response)

